{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, os, random, sys\n",
    "import data\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.token2id('train', 'en') #encode\n",
    "data.token2id('train', 'vi') # decode\n",
    "data.token2id('tst2012', 'en')\n",
    "data.token2id('tst2012', 'vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 29999\n",
      "Bucketing conversation number 39999\n",
      "Bucketing conversation number 49999\n",
      "Bucketing conversation number 59999\n",
      "Bucketing conversation number 69999\n",
      "Bucketing conversation number 79999\n",
      "Bucketing conversation number 89999\n",
      "Bucketing conversation number 99999\n",
      "Bucketing conversation number 109999\n",
      "Bucketing conversation number 119999\n",
      "Bucketing conversation number 129999\n",
      "Number of samples in each bucket:\n",
      " [60747, 28752, 11088, 13801, 8497, 4510]\n",
      "Bucket scale:\n",
      " [0.47683975038266807, 0.7025314965265512, 0.789567879430119, 0.8979002315632482, 0.9645982966364457, 1.0]\n",
      "Initialize new model\n",
      "Create placeholders\n",
      "Create inference\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "Creating loss... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "WARNING:tensorflow:From /anaconda3/envs/muteferrika/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Time: 46.70995378494263\n",
      "Create optimizer... \n",
      "It might take a couple of minutes depending on how many buckets you have.\n",
      "Creating opt for bucket 0 took 11.309788942337036 seconds\n",
      "Creating opt for bucket 1 took 17.277687072753906 seconds\n",
      "Creating opt for bucket 2 took 18.331774950027466 seconds\n",
      "Creating opt for bucket 3 took 24.576195001602173 seconds\n",
      "Creating opt for bucket 4 took 30.155845880508423 seconds\n",
      "Creating opt for bucket 5 took 35.53166389465332 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the NMT \"\"\"\n",
    "test_buckets, data_buckets, train_buckets_scale = _get_buckets()\n",
    "# in train mode, we need to create the backward path, so forwrad_only is False\n",
    "model = TranslationModel(False, config.BATCH_SIZE)\n",
    "model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running session\n",
      "Initializing fresh parameters for the TranslationModel\n",
      "Iter 30: loss 10.139759985605876, time 10.134808778762817\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        print('Running session')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        _check_restore_parameters(sess, saver)\n",
    "\n",
    "        iteration = model.global_step.eval()\n",
    "        total_loss = 0\n",
    "        while True:\n",
    "            skip_step = _get_skip_step(iteration)\n",
    "            bucket_id = _get_random_bucket(train_buckets_scale)\n",
    "            encoder_inputs, decoder_inputs, decoder_masks = data.get_batch(data_buckets[bucket_id], \n",
    "                                                                           bucket_id,\n",
    "                                                                           batch_size=config.BATCH_SIZE)\n",
    "            start = time.time()\n",
    "            _, step_loss, _ = run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, False)\n",
    "            total_loss += step_loss\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % skip_step == 0:\n",
    "                print('Iter {}: loss {}, time {}'.format(iteration, total_loss/skip_step, time.time() - start))\n",
    "                start = time.time()\n",
    "                total_loss = 0\n",
    "                saver.save(sess, os.path.join(config.CPT_PATH, 'chatbot'), global_step=model.global_step)\n",
    "                if iteration % (10 * skip_step) == 0:\n",
    "                    # Run evals on development set and print their loss\n",
    "                    _eval_test_set(sess, model, test_buckets)\n",
    "                    start = time.time()\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_random_bucket(train_buckets_scale):\n",
    "    \"\"\" Get a random bucket from which to choose a training sample \"\"\"\n",
    "    rand = random.random()\n",
    "    return min([i for i in range(len(train_buckets_scale))\n",
    "                if train_buckets_scale[i] > rand])\n",
    "\n",
    "def _assert_lengths(encoder_size, decoder_size, encoder_inputs, decoder_inputs, decoder_masks):\n",
    "    \"\"\" Assert that the encoder inputs, decoder inputs, and decoder masks are\n",
    "    of the expected lengths \"\"\"\n",
    "    if len(encoder_inputs) != encoder_size:\n",
    "        raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                        \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "    if len(decoder_inputs) != decoder_size:\n",
    "        raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "    if len(decoder_masks) != decoder_size:\n",
    "        raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_masks), decoder_size))\n",
    "\n",
    "\n",
    "def run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, forward_only):\n",
    "    \"\"\" Run one step in training.\n",
    "    @forward_only: boolean value to decide whether a backward path should be created\n",
    "    forward_only is set to True when you just want to evaluate on the test set,\n",
    "    or when you want to the bot to be in chat mode. \"\"\"\n",
    "    encoder_size, decoder_size = config.BUCKETS[bucket_id]\n",
    "    _assert_lengths(encoder_size, decoder_size, encoder_inputs, decoder_inputs, decoder_masks)\n",
    "\n",
    "    # input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "    input_feed = {}\n",
    "    for step in range(encoder_size):\n",
    "        input_feed[model.encoder_inputs[step].name] = encoder_inputs[step]\n",
    "    for step in range(decoder_size):\n",
    "        input_feed[model.decoder_inputs[step].name] = decoder_inputs[step]\n",
    "        input_feed[model.decoder_masks[step].name] = decoder_masks[step]\n",
    "\n",
    "    last_target = model.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([model.batch_size], dtype=np.int32)\n",
    "\n",
    "    # output feed: depends on whether we do a backward step or not.\n",
    "    if not forward_only:\n",
    "        output_feed = [model.train_ops[bucket_id],  # update op that does SGD.\n",
    "                       model.gradient_norms[bucket_id],  # gradient norm.\n",
    "                       model.losses[bucket_id]]  # loss for this batch.\n",
    "    else:\n",
    "        output_feed = [model.losses[bucket_id]]  # loss for this batch.\n",
    "        for step in range(decoder_size):  # output logits.\n",
    "            output_feed.append(model.outputs[bucket_id][step])\n",
    "\n",
    "    outputs = sess.run(output_feed, input_feed)\n",
    "    if not forward_only:\n",
    "        return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "    else:\n",
    "        return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "def _check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname(config.CPT_PATH + '/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print(\"Loading parameters for the TranslationModel\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Initializing fresh parameters for the TranslationModel\")\n",
    "\n",
    "def _get_random_bucket(train_buckets_scale):\n",
    "    \"\"\" Get a random bucket from which to choose a training sample \"\"\"\n",
    "    rand = random.random()\n",
    "    return min([i for i in range(len(train_buckets_scale))\n",
    "                if train_buckets_scale[i] > rand])\n",
    "\n",
    "def _get_skip_step(iteration):\n",
    "    \"\"\" How many steps should the model train before it saves all the weights. \"\"\"\n",
    "    if iteration < 100:\n",
    "        return 30\n",
    "    return 100\n",
    "\n",
    "def _get_buckets():\n",
    "    \"\"\" Load the dataset into buckets based on their lengths.\n",
    "    train_buckets_scale is the inverval that'll help us \n",
    "    choose a random bucket later on.\n",
    "    \"\"\"\n",
    "    test_buckets = data.load_data('tst2012_ids.en', 'tst2012_ids.vi')\n",
    "    data_buckets = data.load_data('train_ids.en', 'train_ids.vi')\n",
    "    train_bucket_sizes = [len(data_buckets[b]) for b in range(len(config.BUCKETS))]\n",
    "    print(\"Number of samples in each bucket:\\n\", train_bucket_sizes)\n",
    "    train_total_size = sum(train_bucket_sizes)\n",
    "    # list of increasing numbers from 0 to 1 that we'll use to select a bucket.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in range(len(train_bucket_sizes))]\n",
    "    print(\"Bucket scale:\\n\", train_buckets_scale)\n",
    "    return test_buckets, data_buckets, train_buckets_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel:\n",
    "    def __init__(self, forward_only, batch_size):\n",
    "        \"\"\"forward_only: if set, we do not construct the backward pass in the model.\n",
    "        \"\"\"\n",
    "        print('Initialize new model')\n",
    "        self.fw_only = forward_only\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        # Feeds for inputs. It's a list of placeholders\n",
    "        print('Create placeholders')\n",
    "        self.encoder_inputs = [tf.placeholder(tf.int32, shape=[None], name='encoder{}'.format(i))\n",
    "                               for i in range(config.BUCKETS[-1][0])]\n",
    "        self.decoder_inputs = [tf.placeholder(tf.int32, shape=[None], name='decoder{}'.format(i))\n",
    "                               for i in range(config.BUCKETS[-1][1] + 1)]\n",
    "        self.decoder_masks = [tf.placeholder(tf.float32, shape=[None], name='mask{}'.format(i))\n",
    "                              for i in range(config.BUCKETS[-1][1] + 1)]\n",
    "\n",
    "        # Our targets are decoder inputs shifted by one (to ignore <GO> symbol)\n",
    "        self.targets = self.decoder_inputs[1:]\n",
    "\n",
    "    def _inference(self):\n",
    "        print('Create inference')\n",
    "        # If we use sampled softmax, we need an output projection.\n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if config.NUM_SAMPLES > 0 and config.NUM_SAMPLES < config.DEC_VOCAB:\n",
    "            w = tf.get_variable('proj_w', [config.HIDDEN_SIZE, config.DEC_VOCAB])\n",
    "            b = tf.get_variable('proj_b', [config.DEC_VOCAB])\n",
    "            self.output_projection = (w, b)\n",
    "\n",
    "        def sampled_loss(logits, labels):\n",
    "            labels = tf.reshape(labels, [-1, 1])\n",
    "            return tf.nn.sampled_softmax_loss(weights=tf.transpose(w), \n",
    "                                              biases=b, \n",
    "                                              inputs=logits, \n",
    "                                              labels=labels, \n",
    "                                              num_sampled=config.NUM_SAMPLES, \n",
    "                                              num_classes=config.DEC_VOCAB)\n",
    "        self.softmax_loss_function = sampled_loss\n",
    "\n",
    "        single_cell = tf.contrib.rnn.GRUCell(config.HIDDEN_SIZE)\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell([single_cell for _ in range(config.NUM_LAYERS)])\n",
    "\n",
    "    def _create_loss(self):\n",
    "        print('Creating loss... \\nIt might take a couple of minutes depending on how many buckets you have.')\n",
    "        start = time.time()\n",
    "        def _seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            setattr(tf.contrib.rnn.GRUCell, '__deepcopy__', lambda self, _: self)\n",
    "            setattr(tf.contrib.rnn.MultiRNNCell, '__deepcopy__', lambda self, _: self)\n",
    "            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                    encoder_inputs, decoder_inputs, self.cell,\n",
    "                    num_encoder_symbols=config.ENC_VOCAB,\n",
    "                    num_decoder_symbols=config.DEC_VOCAB,\n",
    "                    embedding_size=config.HIDDEN_SIZE,\n",
    "                    output_projection=self.output_projection,\n",
    "                    feed_previous=do_decode)\n",
    "\n",
    "        if self.fw_only:\n",
    "            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "                                        self.encoder_inputs, \n",
    "                                        self.decoder_inputs, \n",
    "                                        self.targets,\n",
    "                                        self.decoder_masks, \n",
    "                                        config.BUCKETS, \n",
    "                                        lambda x, y: _seq2seq_f(x, y, True),\n",
    "                                        softmax_loss_function=self.softmax_loss_function)\n",
    "            # If we use output projection, we need to project outputs for decoding.\n",
    "            if self.output_projection:\n",
    "                for bucket in range(len(config.BUCKETS)):\n",
    "                    self.outputs[bucket] = [tf.matmul(output, \n",
    "                                            self.output_projection[0]) + self.output_projection[1]\n",
    "                                            for output in self.outputs[bucket]]\n",
    "        else:\n",
    "            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "                                        self.encoder_inputs, \n",
    "                                        self.decoder_inputs, \n",
    "                                        self.targets,\n",
    "                                        self.decoder_masks,\n",
    "                                        config.BUCKETS,\n",
    "                                        lambda x, y: _seq2seq_f(x, y, False),\n",
    "                                        softmax_loss_function=self.softmax_loss_function)\n",
    "        print('Time:', time.time() - start)\n",
    "\n",
    "    def _creat_optimizer(self):\n",
    "        print('Create optimizer... \\nIt might take a couple of minutes depending on how many buckets you have.')\n",
    "        with tf.variable_scope('training') as scope:\n",
    "            self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "            if not self.fw_only:\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(config.LR)\n",
    "                trainables = tf.trainable_variables()\n",
    "                self.gradient_norms = []\n",
    "                self.train_ops = []\n",
    "                start = time.time()\n",
    "                for bucket in range(len(config.BUCKETS)):\n",
    "                    \n",
    "                    clipped_grads, norm = tf.clip_by_global_norm(tf.gradients(self.losses[bucket], \n",
    "                                                                 trainables),\n",
    "                                                                 config.MAX_GRAD_NORM)\n",
    "                    self.gradient_norms.append(norm)\n",
    "                    self.train_ops.append(self.optimizer.apply_gradients(zip(clipped_grads, trainables), \n",
    "                                                            global_step=self.global_step))\n",
    "                    print('Creating opt for bucket {} took {} seconds'.format(bucket, time.time() - start))\n",
    "                    start = time.time()\n",
    "\n",
    "\n",
    "    def _create_summary(self):\n",
    "        pass\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._inference()\n",
    "        self._create_loss()\n",
    "        self._creat_optimizer()\n",
    "        self._create_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
